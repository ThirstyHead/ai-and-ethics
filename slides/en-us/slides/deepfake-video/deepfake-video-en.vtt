WEBVTT

1
00:00:00.729 --> 00:00:06.089
“We’re entering an era in which our enemies
can make anyone say anything at any point

2
00:00:06.089 --> 00:00:07.089
in time.”

3
00:00:07.089 --> 00:00:11.699
Jordan Peele created this fake video of President
Obama to demonstrate how easy it was to put

4
00:00:11.699 --> 00:00:13.309
words in someone else’s mouth-

5
00:00:13.309 --> 00:00:17.190
moving forward we need to be more vigilant
with what we trust from the internet.

6
00:00:17.190 --> 00:00:22.170
not everyone bought it, but the technology
behind it is rapidly improving, even as worries

7
00:00:22.170 --> 00:00:24.669
increase about its potential for harm.

8
00:00:24.669 --> 00:00:28.449
This is your Bloomberg QuickTake on Fake Videos.

9
00:00:28.449 --> 00:00:32.580
Deep fakes, or realistic-looking fake videos
and audio, gained popularity as a means of

10
00:00:32.580 --> 00:00:34.960
adding famous actresses into porn scenes.

11
00:00:34.960 --> 00:00:38.720
Despite bans on major websites, they remain
easy to make and find.

12
00:00:38.720 --> 00:00:43.760
They’re named for the deep-learning AI algorithms
that make them possible.

13
00:00:43.760 --> 00:00:48.739
Input real audio or video of a specific person-
the more, the better- and the software tries

14
00:00:48.739 --> 00:00:51.500
to recognize patterns in speech and movement.

15
00:00:51.500 --> 00:00:55.180
Introduce a new element like someone else’s
face or voice, and a deep fake is born.

16
00:00:55.180 --> 00:00:59.920
Jeremy Kahn: It's actually extremely easy
to make one of these things… there was just

17
00:00:59.920 --> 00:01:04.629
some breakthroughs from academic researchers
who work with this particular kind of machine

18
00:01:04.629 --> 00:01:08.850
learning in the past few weeks, which would
drastically reduce the amount of video you

19
00:01:08.850 --> 00:01:10.410
need actually to create one of these.

20
00:01:10.410 --> 00:01:15.860
Programs like FakeApp, the most popular one
for making deep fakes, need dozens of hours

21
00:01:15.860 --> 00:01:20.980
of human assistance to create a video that
looks like this rather than this, but that’s

22
00:01:20.980 --> 00:01:21.980
changing.

23
00:01:21.980 --> 00:01:24.930
In September researchers at Carnegie-Mellon
revealed unsupervised software that accurately

24
00:01:24.930 --> 00:01:33.750
reproduced not just facial features, but changing
weather patterns and flowers in bloom as well.

25
00:01:33.750 --> 00:01:36.570
But with increasing capability comes increasing
concern.

26
00:01:36.570 --> 00:01:39.270
You know, this is kind of fake news on steroids
potentially.

27
00:01:39.270 --> 00:01:45.360
We do not know of a case yet where someone
has tried to use this to perpetrate a kind

28
00:01:45.360 --> 00:01:52.650
of fraud or an information warfare campaign,
or for that matter, to really damage someone’s

29
00:01:52.650 --> 00:01:56.230
reputation// but it’s the danger that everyone
is really afraid of.

30
00:01:56.230 --> 00:02:01.860
In a world where fakes are easy to create-
authenticity also becomes easier to deny.

31
00:02:01.860 --> 00:02:05.320
People caught doing genuinely objectionable
things could claim evidence against them is

32
00:02:05.320 --> 00:02:06.590
bogus.

33
00:02:06.590 --> 00:02:11.910
Fake videos are also difficult to detect,
though researchers and the US Department of

34
00:02:11.910 --> 00:02:14.739
Defense, in particular, have said they’re
working on ways to counter them.

35
00:02:14.739 --> 00:02:20.159
Deep Fakes do however have some positive potential-
take CereProc, who creates digital voices

36
00:02:20.159 --> 00:02:26.849
for people who lose theirs from disease…

37
00:02:26.849 --> 00:02:31.900
There are also applications that could be
considered more value-neutral, like the many,

38
00:02:31.900 --> 00:02:37.400
many deep fakes that exist solely to turn
as many movies as possible into Nicolas Cage

39
00:02:37.400 --> 00:02:39.019
movies.